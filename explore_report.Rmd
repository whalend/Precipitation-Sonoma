---
title: "Rain gauge data exploration"
author: "whalen"
date: "November 10, 2014"
output: html_document
---

# Rain gauge data summary and manipulation
#### Goal: summarize precipitation events into hourly and daily, and convert to inches
- Conversion factor: 1 event = 0.01 inches


```{r Load libraries}
library(plyr)
library(data.table)
library(dplyr)
library(tidyr)
```

NOT RUN: Manipulations of some single files that were not correctly formatted for opening in Excel. I later found a better way to do this using the CSV library in Python that allows relatively simple conversion of text/CSV files from one delimiter to another.
```{r not run, eval=FALSE, echo=FALSE}
# d1 <- fread("Rain_Gauge/all_years_raw_exports/SDC_502836_2007_RG.csv")
# d1 <- separate(d1, date_time, c("date", "time"), sep=" ")
# write.csv(d1, "Rain_Gauge/all_years_raw_exports/SDC_502836_2007_RG.csv")
# 
# d1 <- fread("Rain_Gauge/all_years_raw_exports/SDC_502836_2008_RG.csv")
# d1$id <- "sec"
# write.csv(d1, "Rain_Gauge/all_years_raw_exports/SDC_502836_2008_RG.csv")
# 
# d1 <- fread("Rain_Gauge/all_years_raw_exports/SUGAR_20070416_20090411_RG.csv")
# d1$id <- "sugarloaf"
# d1 <- d1 %>% separate(date_time, c("date", "time"), sep=" ") %>% 
#       select(id, date, time, events)
# write.csv(d1, "Rain_Gauge/all_years_raw_exports/SUGAR_20070416_20090411_RG.csv")
# 
# d1 <- fread("Rain_Gauge/all_years_raw_exports/YAHNG_502848_2009_RG.txt")
# names(d1)[1:2]  <- c("dt", "events")
# d1$id <- "yahng"
# d1 <- d1 %>% separate(dt, c("date", "time"), sep=" ") %>% 
#       select(id, date, time, events)
# write.csv(d1, "Rain_Gauge/all_years_raw_exports/YAHNG_502848_2009_RG.csv")
# head(d1)
# 
# d1 <- fread("Rain_Gauge/all_years_raw_exports/MITSUI_502950_2006_RG.csv")
# head(d1)
# d1$id <- "mitsui"
# write.csv(d1, "Rain_Gauge/all_years_raw_exports/MITSUI_502950_2006_RG.csv")
```

NOT RUN: Summarizing independent files of four plots to daily and hourly because they were not in the .hobo or .dtf format of the Onset data logger files. Individual files for daily and hourly were then exported as CSV, so there are two files for these locations that will need to be merged into the full database.
```{r not run 2, eval=FALSE, echo=FALSE}
library(xlsx)

badg <- read.xlsx("Rain_Gauge/all_years_raw/BADGER_669845_2004_RG_B.xlsx", 1)
head(badg)
badg$date_time2 <- strptime(badg$date_time, format = "%m/%d/%y %H:%M")
badg$year <- year(badg$date_time2)
badg$month <- month(badg$date_time2)
badg$day <- mday(badg$date_time2)
badg$hour <- hour(badg$date_time2)

badg_daily <- badg %>% 
      select(date_time, year, month, day, hour, events) %>% 
      group_by(year, month, day) %>%
      summarize(daily_events=length(events), daily_ppt=length(events)*0.01)

badg_daily$id <- "badger"
badg_daily <- select(badg_daily, id, year, month, day, daily_events, daily_ppt)
head(badg_daily)

badg_hourly <- badg %>% 
      select(date_time, year, month, day, hour, events) %>% 
      group_by(year, month, day, hour) %>%
      summarize(hourly_events=length(events), hourly_ppt=length(events)*0.01)

badg_hourly$id <- "badger"
badg_hourly <- select(badg_hourly, id, year, month, day, hour, hourly_events, hourly_ppt)
head(badg_hourly)


sec <- read.xlsx("SDC_502836_2007_RG.xls", 1)
head(sec)
sec <- select(sec, date_time = Date.Time, events = Event..Events.)
sec$date_time2 <- strptime(sec$date_time, format = "%m/%d/%y %H:%M")
sec$year <- year(sec$date_time2)
sec$month <- month(sec$date_time2)
sec$day <- mday(sec$date_time2)
sec$hour <- hour(sec$date_time2)

sec_daily <- sec %>% 
      select(date_time, year, month, day, hour, events) %>% 
      group_by(year, month, day) %>%
      summarize(daily_events=length(events), daily_ppt=length(events)*0.01)

sec_daily$id <- "sec"
sec_daily <- select(sec_daily, id, year, month, day, daily_events, daily_ppt)
head(sec_daily)

sec_hourly <- sec %>%
      select(date_time, year, month, day, hour, events) %>% 
      group_by(year, month, day, hour) %>%
      summarize(hourly_events=length(events), hourly_ppt=length(events)*0.01)

sec_hourly$id <- "sec"
sec_hourly <- select(sec_hourly, id, year, month, day, hour, hourly_events, hourly_ppt)
head(sec_hourly)


sugar <- read.xlsx("SUGAR_20070416_20090411_RG.xlsx", 1)
head(sugar)
sugar$date_time2 <- strptime(sugar$date_time, format = "%m/%d/%y %H:%M")
sugar$year <- year(sugar$date_time2)
sugar$month <- month(sugar$date_time2)
sugar$day <- mday(sugar$date_time2)
sugar$hour <- hour(sugar$date_time2)

sugar_daily <- sugar %>% 
      select(date_time, year, month, day, hour, events) %>% 
      group_by(year, month, day) %>%
      summarize(daily_events=length(events), daily_ppt=length(events)*0.01)

sugar_daily$id <- "sugarloaf"
sugar_daily <- select(sugar_daily, id, year, month, day, daily_events, daily_ppt)
head(sugar_daily)

sugar_hourly <- sugar %>%
      select(date_time, year, month, day, hour, events) %>% 
      group_by(year, month, day, hour) %>%
      summarize(hourly_events=length(events), hourly_ppt=length(events)*0.01)
sugar_hourly$id <- "sugarloaf"
sugar_hourly <- select(sugar_hourly, id, year, month, day, hour, hourly_events, hourly_ppt)
head(sugar_hourly)


yahng <- fread("YAHNG_502848_2009_RG.txt")
head(yahng)
str(yahng)
yahng <- as.data.frame(select(yahng, date_time = 1, events = 2))
yahng$date_time2 <- strptime(yahng$date_time, format = "%m/%d/%y %H:%M")
yahng$year <- year(yahng$date_time2)
yahng$month <- month(yahng$date_time2)
yahng$day <- mday(yahng$date_time2)
yahng$hour <- hour(yahng$date_time2)

yahng_daily <- yahng %>% 
      select(date_time, year, month, day, hour, events) %>% 
      group_by(year, month, day) %>%
      summarize(daily_events=length(events), daily_ppt=length(events)*0.01)

yahng_daily$id <- "yahng"
yahng_daily <- select(yahng_daily, id, year, month, day, daily_events, daily_ppt)
head(yahng_daily)

yahng_hourly <- yahng %>%
      select(date_time, year, month, day, hour, events) %>% 
      group_by(year, month, day, hour) %>%
      summarize(hourly_events=length(events), hourly_ppt=length(events)*0.01)
yahng_hourly$id <- "yahng"
yahng_hourly <- select(yahng_hourly, id, year, month, day, hour, hourly_events, hourly_ppt)
head(yahng_hourly)



write.csv(badg_daily, "Rain_Gauge/2_RG_EXPORTS/special_cases/badger_day_2004.csv")
write.csv(badg_hourly, "Rain_Gauge/2_RG_EXPORTS/special_cases/badger_hr_2004.csv")
write.csv(sec_daily, "Rain_Gauge/2_RG_EXPORTS/special_cases/sec_day_2007.csv")
write.csv(sec_hourly, "Rain_Gauge/2_RG_EXPORTS/special_cases/sec_hr_2007.csv")
write.csv(sugar_daily, "Rain_Gauge/2_RG_EXPORTS/special_cases/sugar_day_2007_2009.csv")
write.csv(sugar_hourly, "Rain_Gauge/2_RG_EXPORTS/special_cases/sugar_hr_2007_2009.csv")
write.csv(yahng_daily, "Rain_Gauge/2_RG_EXPORTS/special_cases/yahng_day_2009.csv")
write.csv(yahng_hourly, "Rain_Gauge/2_RG_EXPORTS/special_cases/yahng_hr_2009.csv")
```


Files that were "special cases" because they did not have a HoboWare compatible data file. I am reading these into `R` and then creating two dataframes, one that is hourly and one that is daily. My plan is to break apart the larger dataframe read in below into a daily and hourly dataframe and then bind those to each of these.
```{r Read in single files, echo=FALSE, eval=FALSE}
# Yahng 2009 files
y_hr09 <- fread("Rain_Gauge/2_RG_EXPORTS/special_cases/yahng_hr_2009.csv") 
y_dy09 <- fread("Rain_Gauge/2_RG_EXPORTS/special_cases/yahng_day_2009.csv")

# Sugarloaf 2007-2009 files
s_hr07 <- fread("Rain_Gauge/2_RG_EXPORTS/special_cases/sugar_hr_2007_2009.csv")
s_dy07 <- fread("Rain_Gauge/2_RG_EXPORTS/special_cases/sugar_day_2007_2009.csv")

# Sonoma ecology center 2007 files
sec_hr07 <- fread("Rain_Gauge/2_RG_EXPORTS/special_cases/sec_hr_2007.csv")
sec_dy07 <- fread("Rain_Gauge/2_RG_EXPORTS/special_cases/sec_day_2007.csv")

#Badger 2004 files
ba_hr04 <- fread("Rain_Gauge/2_RG_EXPORTS/special_cases/badger_hr_2004.csv")
ba_dy04 <- fread("Rain_Gauge/2_RG_EXPORTS/special_cases/badger_day_2004.csv")
```

```{r bind special cases, echo=FALSE, eval=FALSE}
daily_special <- rbind.fill(list(y_dy09,s_dy07,sec_dy07,ba_dy04))
tail(daily_special)
daily_special <- daily_special[,-1]# Drop the first column as excess variable
daily_special$date <- with(daily_special, paste(month, day, year, sep="/"))
daily_special <- daily_special %>%
      select(id, date, year, month, day, daily_events, daily_ppt)

hourly_special <- rbind.fill(list(y_hr09,s_hr07,sec_hr07,ba_hr04))
tail(hourly_special)
hourly_special <- hourly_special[,-1]# Drop the first column as excess variable
hourly_special$date <- with(hourly_special, paste(month, day, year, sep="/"))
hourly_special <- hourly_special %>% 
      select(id, date, year, month, day, hour, hourly_events, hourly_ppt)
```

Remove the single file data frames from the environment
```{r remove data frames, eval=FALSE, echo=FALSE}
rm(y_hr09)
rm(y_dy09)
rm(sec_hr07)
rm(sec_dy07)
rm(s_dy07)
rm(s_hr07)
rm(ba_hr04)
rm(ba_dy04)
```

Make database by compiling all the files that were exported using the proprietary HoboWare software together. This will result in one large dataframe that contains hourly and daily events. I will then want to break this down further into separate hourly and daily dataframes
```{r List files in working directory, eval=TRUE, echo=TRUE}
#setwd("~/Documents/git_local/soco_ppt")
files <- list.files("Rain_Gauge/2_RG_EXPORTS", pattern="*.csv", full.names=TRUE)
# fnames <- sub("^([^.]*).*", "\\1", basename(files)) # create list of file basenames
# gauge_names <- sub("^([^_.]*).*", "\\1", basename(files)) # create list of gauge names by pulling the first part of the file name
# gname <- sub("^([^_.]*).*", "\\1", basename(files)) # same as above
```

Read all the files in the list "files" into a single data frame. 
```{r Batch read in files}
rg_data <- ldply(files, function(i){fread(i)})

# rg_data <- ldply(files, function(i){
#       read.csv(i, skip=2, stringsAsFactors=FALSE, col.names = c("date", "time","events","daily_events","hourly_events"))
#       # fnames <- sub("^([^.]*).*", "\\1", basename(i))
#       gauge_name <- sub("^([^_.]*).*", "\\1", basename(i))
#       i$id <- gauge_name
# })
head(rg_data)
str(rg_data)
```

Join `date` and `time` columns in `rg_data` and convert to POSIX
```{r POSIX time conversion}
rg_data$date_time <- paste(rg_data$date, rg_data$time, sep=" ")
rg_data$date_time <- strptime(rg_data$date_time, format="%m/%d/%Y %H:%M:%S", tz="UTC")
```

Create year, month, day, and hour columns for grouping
```{r create date variables, echo=TRUE}
rg_data$date1 <- as.Date(rg_data$date, format = "%m/%d/%Y")
rg_data$year <- year(rg_data$date1)
rg_data$month <- month(rg_data$date1)
rg_data$day <- mday(rg_data$date1)
rg_data$hour <- hour(rg_data$date_time)
head(rg_data)
summary(rg_data)
class(rg_data$date_time)# Confirm POSIX format
saveRDS(rg_data, file = "rg_data.rds")

## Code used for troubleshooting date/time problems stemming from CSV files
#d1 <- subset(rg_data, date1 == "0004-04-30")
#unique(d1$id)
#summary(rg_data$date1)
#min(rg_data$year)
```

Summarize `rg_data` into a data set with hourly resolution. There must be a way to do this based on the POSIX date/time format, which might be more efficient. This relies on `dplyr`
```{r create hourly data set, echo=TRUE}
library(dplyr)
rg_data <- readRDS("rg_data.rds")
rg_data1  <- rg_data
rg_data1[is.na(rg_data1)] <- 0

hr_rg_data <- rg_data %>% 
      select(id, year, month, day, hour, hourly_events) %>% 
      group_by(id, year, month, day, hour) %>%
      summarise(hourly_ppt=sum(hourly_events, na.rm = TRUE) *0.01)
hr_rg_data
# class(hr_rg_data)
summary(hr_rg_data)

hr_rg_data1 <- rg_data1 %>%
      select(id, year, month, day, hour, hourly_events) %>% 
      group_by(id, year, month, day, hour) %>%
      summarise(hourly_ppt=sum(hourly_events)*0.01)
hr_rg_data1
summary(hr_rg_data1)

#fix(hr_rg_data1)
```

This grouping and summarization has eliminated `NA`s from the precipiation variables in the resulting data frame. Next I will `rbind` it with the `hourly_special` dataframe and save it as an R data source file
```{r join hourly dataframes, echo=TRUE}
hr_special <- select(hourly_special, -date, -hourly_events)
hr_rg_data <- rbind(hr_rg_data, hr_special)
hr_rg_data
summary(hr_rg_data)
saveRDS(hr_rg_data, file = "hr_rg_data.rds")
hr_rg_data <- readRDS("hr_rg_data.rds")
hr_rg_data$date <- as.Date(paste(hr_rg_data$year, hr_rg_data$month, hr_rg_data$day, sep = "/"), format = "%Y/%m/%d")
summary(hr_rg_data)

hr_rg_data1 <- rbind(hr_rg_data1, hr_special)
hr_rg_data1$date <- as.Date(paste(hr_rg_data1$year, hr_rg_data1$month, hr_rg_data1$day, sep = "/"), format = "%Y/%m/%d")
summary(hr_rg_data1)
```
Now I have one dataframe with all the hourly events recorded across all of the rain gauges. Next I will follow the same process to create a dataframe of all the daily events.
```{r create daily data set, echo=TRUE}
dy_rg_data <- rg_data %>% 
      select(id, year, month, day, hourly_events) %>% 
      group_by(id, year, month, day) %>%
      summarize(daily_ppt=sum(hourly_events, na.rm = TRUE)*0.01)
dy_rg_data
summary(dy_rg_data)
#dy_rg_data$date <- as.Date(dy_rg_data$date, format = "%m/%d/%Y")
#arrange(dy_rg_data, desc(date))# there is a date getting formatted with a year of 0201. I need to replace that with year 2010.
# class(dy_rg_data)
dy_rg_data1 <- rg_data1 %>%
      select(id, year, month, day, hourly_events) %>% 
      group_by(id, year, month, day) %>%
      summarize(daily_ppt=sum(hourly_events)*0.01)
dy_rg_data1
summary(dy_rg_data1)
```

Next, `rbind` this with the `daily_special` dataframe and save it as an R data source file
```{r join daily dataframes, echo=TRUE}
dy_special <- select(daily_special, -date, -daily_events)
dy_rg_data <- rbind(dy_rg_data, dy_special)
dy_rg_data
summary(dy_rg_data)
saveRDS(dy_rg_data, file = "dy_rg_data.rds")
dy_rg_data <- readRDS("dy_rg_data.rds")
dy_rg_data$date <- as.Date(paste(dy_rg_data$year, dy_rg_data$month, dy_rg_data$day, sep = "/"), format="%Y/%m/%d")

dy_rg_data1 <- rbind(dy_rg_data1, dy_special)
dy_rg_data1$date <- as.Date(paste(dy_rg_data1$year, dy_rg_data1$month, dy_rg_data1$day, sep = "/"), format="%Y/%m/%d")
dy_rg_data1
summary(dy_rg_data1)
```


Looking at the summaries it appears something rather sketchy is going on. The maximum values are way above a realistic range, with a maximum daily precipitation of over 52 inches and a maximum of hourly precipitation of over 14 inches. **Just. Not. Possible.** 

Exploring when and where these enormous anomalous values are coming from, first dealing with the daily events data frame. For reference, this pdf has some data on maximum rainfall recorded in California <http://www.cepsym.org/Sympro1994/goodridge_paper.pdf> Also more information here: <http://earthzine.org/2011/04/19/average-recurrence-interval-of-extreme-rainfall-in-real-time/> which I believe refers to the report from the first link.
```{r explore outliers, eval=FALSE, echo=FALSE}
library(plyr)
library(dplyr)
filter(dy_rg_data, daily_ppt>6.5)
filter(dy_rg_data, year==2007 & month==1 & day==28)
filter(dy_rg_data, year==2007 & month==1 & day==27)
filter(dy_rg_data, year==2008 & month==3 & day==15)

qplot(date, daily_ppt, data=dy_rg_data, color = id)
qplot(date, daily_ppt, data=dy_rg_data %>% filter(daily_ppt < 6.5), color = id)
qplot(date, hourly_ppt, data=hr_rg_data, color = id)
qplot(date, hourly_ppt, data=hr_rg_data %>% filter(hourly_ppt < 2), color = id)

filter(hr_rg_data, hourly_ppt>2)
filter(hr_rg_data, year==2008 & month==3 & day==15 & hour==18)

library(data.table)
sr_data <- fread("http://web1.ci.santa-rosa.ca.us/pworks/other/weather/download%207-1-03%206-30-04.txt")
head(sr_data)
summary(sr_data)
sum(sr_data$V15)
sr_data$date <- as.Date(sr_data$V1, format = "%m/%d/%Y")
qplot(date, daily_ppt, data = sr_data %>% group_by(date) %>% summarise(daily_ppt = sum(V15)))
```
I was thinking that maybe I should replace the outlying values with the average of the remaining values for that day or hour...but maybe not.

Francesco talking over the exploration with me
```{r explore with francesco, eval=FALSE, echo=FALSE}
test=dy_rg_data$date2[order(dy_rg_data$date2), ]
test=dy_rg_data[order(dy_rg_data$date2), ]
head(test)
head(test,40)
test=dy_rg_data[order(dy_rg_data$date2,dy_rg_data$id), ]
```

Francesco suggests that for aggregating to monthly, If more than 5 days of `NA`/missing for a month then treat entire month as missing. Maybe use a spline approach instead of regression kriging because of small number of points (FT, suggests thin plate spline).

For outlier replacement I settled on a daily precipitation threshold of greater than or equal to 6.5 inches, and an hourly precipitation threshold of greater than or equal to 2 inches.
```{r outliers to NA, eval=FALSE, echo=TRUE}
library(ggplot2)
library(dplyr)
# Replace ppt values >= 6.5 with `NA`
dy_rg_data$daily_ppt[dy_rg_data$daily_ppt>=6.5] <- NA
summary(dy_rg_data)
saveRDS(dy_rg_data, "dy_rg_data_corrected.rds")
dy_rg_data1$daily_ppt[dy_rg_data1$daily_ppt >= 6.5] <- NA
summary(dy_rg_data1)

qplot(as.numeric(year), daily_ppt, data = dy_rg_data %>% 
            group_by(id, year, month, day), color = id) # + 
      # geom_text(aes(label=id),hjust=-0.1, vjust=0)
qplot(date, daily_ppt, data = dy_rg_data, color = id)

# Looking at precipitation by month during 2006
qplot(month, monthly_ppt, data = dy_rg_data %>% 
            filter(year==2006) %>%
            group_by(id, year, month) %>%
            summarize(monthly_ppt=sum(daily_ppt)), 
      color = id, main = "Monthly total from daily data - 2006") #+
      #geom_point(shape = 5, size = 1.5)

# An implementation of the `qplot` above using `ggplot`
ggplot(data = dy_rg_data %>% 
            filter(year==2006) %>%
            group_by(id, year, month) %>%
            summarize(monthly_ppt=sum(daily_ppt)),
       aes(x = month, y = monthly_ppt, color = id)) +
      geom_point(size=2, shape=3)

#Replace hourly_ppt values >= 2.5 
hr_rg_data$hourly_ppt[hr_rg_data$hourly_ppt>=2]  <- NA
summary(hr_rg_data)
saveRDS(hr_rg_data, "hr_rg_data_corrected.rds")
hr_rg_data <- readRDS("hr_rg_data_corrected.rds")

qplot(date, hourly_ppt, data = hr_rg_data, color = id)

qplot(month, monthly_ppt, data = hr_rg_data %>%
            group_by(id, year, month) %>%
            filter(year == 2006) %>%
            summarize(monthly_ppt=sum(hourly_ppt)), color = id,
      main="Monthly total from hourly data - 2006")
```

```{r FOP weather data 1997-2005}
fop <- read.csv("FOP_weather_data_1997_2005.csv", na.strings = c(".", "", ". "))
summary(fop)
str(fop)
fop$date <- as.Date(fop$Date, format = "%m/%d/%Y")
library(data.table)
fop$year <- year(fop$date)
fop$month <- month(fop$date)
fop$day <- mday(fop$date)
str(fop)
library(dplyr)
fop <- as.tbl(fop)
fop$id <- "fop"

fop_ppt <- fop %>%
      select(id, year, month, day, Precip_cm, Remarks, date)
str(fop_ppt)
fop_ppt$daily_ppt <- fop_ppt$Precip_cm*0.39370 #convert precip in cm to inches
fop_ppt$daily_ppt[fop_ppt$daily_ppt >= 6.5] <- NA #presumed outliers to NA
filter(fop_ppt, Remarks != "snow")
fop_ppt$daily_ppt[fop_ppt$Remarks != "snow"] <- NA
summary(fop_ppt)

fop_ppt_2003_2005 <- fop_ppt %>% 
      select(-Precip_cm, -Remarks) %>% 
      filter(year > 2002)
summary(fop_ppt_2003_2005)
# Match column order to daily rain gauge data from all other locations
dy_rg_data 
fop_ppt_2003_2005 <- select(fop_ppt_2003_2005, id, year, month, day, daily_ppt, date)
summary(fop_ppt_2003_2005)
# Add FOP rainfall data to all other daily data
dy_rg_data <- rbind(dy_rg_data, fop_ppt_2003_2005)
summary(dy_rg_data)
saveRDS(dy_rg_data, "dy_rg_data_corrected.rds")
```

Examine/add precipitation data from other stations in Sonoma County. These data are station summaries from the [Community Collaborative Rain, Hail, and Snow Network](http://www.cocorahs.org). I need to overlay them on the elevation grid to see if they reasonably fall within our study area.

```{r precip other stations}
library(readxl)
ppt_other <- read_excel("ppt_other_stations.xlsx", 1)
summary(ppt_other)
str(ppt_other)

library(rgdal)
ppt_other_sp <- SpatialPointsDataFrame(
    as.matrix(ppt_other[, c("lon","lat")]), as.data.frame(ppt_other),
    proj4string=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
)

elev <- raster("dem/sod15m.tif")
crs <- elev@crs

ppt_other_sp <- spTransform(ppt_other_sp, CRS=crs)

str(ppt_other_sp)
plot(elev)
plot(ppt_other_sp, add = T, pch = 25, col="blue")
# plot(plts, add = T, pch = 0)
```

It looks like the Rohnert Park & Santa Rosa stations are outside the area, so I create a shape file with the locations of the Glen Ellen rain gauges.

```{r other ppt create shapefile}
library(dplyr)
ppt_other <- filter(ppt_other, station == "GlenEllen" | station == "GlenEllenWNW")
unique(ppt_other$station)
summary(ppt_other)
glen_ellen_sp <- ppt_other %>% distinct(station) %>% select(station, lat, lon)

glen_ellen_rg <- SpatialPointsDataFrame(
    as.matrix(glen_ellen_sp[, c("lon","lat")]), 
    as.data.frame(glen_ellen_sp),
    proj4string=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
)
glen_ellen_rg <- spTransform(glen_ellen_rg, CRS=crs)
summary(glen_ellen_rg)

plot(elev)
plot(glen_ellen_rg, add = T, pch = 25, col = "blue")
plot(rg_2003_2008, add = T, pch = 20, col = "green")
# plot(rainy06, add = T, pch = 5, col = "black")
# plot(plts, add = T, pch = 0)

writeOGR(glen_ellen_rg, "shapefiles/", "GlenEllen_ppt_2009-2015", driver = "ESRI Shapefile", overwrite_layer = T)
```

Now I'm going to create a data frame of the Glen Ellen stations formatted to merge with the `dy_rg_data` data frame

```{r Glen Ellen data frame}
summary(ppt_other)
unique(ppt_other$station)
names(dy_rg_data)

ppt_other$date <- as.Date(ppt_other$Date)
ppt_other <- mutate(ppt_other, daily_ppt = Precip_mm/25.4)
str(ppt_other)
summary(ppt_other)

ppt_other <- ppt_other %>% select(station, date, daily_ppt)

library(lubridate)
ppt_other$year <- year(ppt_other$date)
ppt_other$month <- month(ppt_other$date)
ppt_other$day <- day(ppt_other$date)
detach("package:lubridate", unload=TRUE)

ppt_other
dy_rg_data
ppt_other <- ppt_other %>% 
      select(station, year, month, day, daily_ppt) %>% 
      mutate(date = as.Date(paste(year, month, day, sep = "-"))) %>%
      rename(id = station)
ppt_other$id <- tolower(ppt_other$id)
dy_rg_data <- rbind(dy_rg_data, ppt_other)
saveRDS(dy_rg_data, "dy_rg_data_corrected.rds")
```

